{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Dynamic Programming ##\n",
    "# bottom up - efficient\n",
    "def fib1(n):\n",
    "    a = []\n",
    "    for i in range(n):\n",
    "        if i>=2:\n",
    "            a.append(a[i-1]+a[i-2])\n",
    "        else:\n",
    "            a.append(1)\n",
    "    return a[n-1]\n",
    "\n",
    "# recursive calls - lazy\n",
    "def fib2(n):\n",
    "    if n==1 or n==2:\n",
    "        return 1\n",
    "    else:\n",
    "        return fib2(n-1)+fib2(n-2)\n",
    "\n",
    "print(fib1(35))\n",
    "fib2(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/TaMmY/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, glob\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from __future__ import print_function\n",
    "from BeautifulSoup import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml = bs(open('dictionary.xml').read())\n",
    "train= open('train.data').read().split('\\n')\n",
    "validate= open('validate.data').read().split('\\n')\n",
    "test= open('test.data').read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Lex Model - Augmenting dictionary using Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentXML(train, xml):\n",
    "    for i in train:\n",
    "        try:\n",
    "            feature = i.split(' | ')\n",
    "            train_word = feature[0]\n",
    "            train_sense = feature[1]\n",
    "            train_context = ''.join(feature[2].split('%% ')) # removing %% around target word\n",
    "\n",
    "            # looking up the target word in train data inside the xml dict\n",
    "            for j in xml.findAll('lexelt',{'item':train_word}):\n",
    "\n",
    "                # looking for sense inside the target word xml tag and adding context to it\n",
    "                for k in j.findAll('sense',{'id':train_sense}):\n",
    "                    k['examples'] = k['examples']+' | '+train_context\n",
    "                    k['examples'] = os.path.join(k['examples'])\n",
    "\n",
    "        except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    ## Writing the newly augmented dictionary\n",
    "    f = open('augmentedDictionary.xml', \"w\")\n",
    "    f.write(augDict.prettify())\n",
    "    f.close()  \n",
    "    return xml\n",
    "\n",
    "# augXml = augmentXML(train, xml)\n",
    "augXml = bs(open('augmentedDictionary.xml').read())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesk Sense Using XML dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leskSense_xml(context_sentence, ambiguous_word, xml_item, corpus='gloss' , stem=True):\n",
    "    max_overlaps = 0; lesk_sense = None\n",
    "    context_sentence = context_sentence.split()\n",
    "\n",
    "    for item in xml_item.findAll('sense'):\n",
    "        \n",
    "        item_dict = dict(item.attrs)\n",
    "        lesk_dictionary = []\n",
    "\n",
    "        # Includes gloss=definition or examples.\n",
    "        lesk_dictionary+= item_dict[corpus].split()\n",
    "        \n",
    "        if stem == True: # Matching exact words causes sparsity, so lets match stems.\n",
    "            lesk_dictionary = [ps.stem(i) for i in lesk_dictionary]\n",
    "            context_sentence = [ps.stem(i) for i in context_sentence] \n",
    "        \n",
    "        # gives all the possible overlaps of single, consecutive, or more overlaps.\n",
    "        overlaps = set(lesk_dictionary).intersection(context_sentence)\n",
    "\n",
    "        if len(overlaps) > max_overlaps:\n",
    "            lesk_sense = item\n",
    "            max_overlaps = len(overlaps)\n",
    "    \n",
    "    if lesk_sense is None:\n",
    "        return 0, max_overlaps\n",
    "    else:\n",
    "        return str(lesk_sense.attrs[0][1]), max_overlaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesk Sense Using Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import PorterStemmer\n",
    "from itertools import chain\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def leskSense_wn(context_sentence, ambiguous_word, stem=True, hyperhypo=True):\n",
    "    max_overlaps = 0; lesk_sense = None\n",
    "    context_sentence = context_sentence.split()\n",
    "    for ss in wn.synsets(ambiguous_word):\n",
    "        \n",
    "        lesk_dictionary = []\n",
    "\n",
    "        # Includes definition.\n",
    "        lesk_dictionary+= ss.definition().split()\n",
    "        # Includes lemma_names.\n",
    "        lesk_dictionary+= ss.lemma_names()\n",
    "\n",
    "        # Optional: includes lemma_names of hypernyms and hyponyms.\n",
    "        if hyperhypo == True:\n",
    "            lesk_dictionary+= list(chain(*[i.lemma_names() for i in ss.hypernyms()+ss.hyponyms()]))       \n",
    "\n",
    "        if stem == True: # Matching exact words causes sparsity, so lets match stems.\n",
    "            lesk_dictionary = [ps.stem(i) for i in lesk_dictionary]\n",
    "            context_sentence = [ps.stem(i) for i in context_sentence] \n",
    "\n",
    "        overlaps = set(lesk_dictionary).intersection(context_sentence)\n",
    "\n",
    "        if len(overlaps) > max_overlaps:\n",
    "            lesk_sense = ss\n",
    "            max_overlaps = len(overlaps)\n",
    "\n",
    "    return lesk_sense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensePrediction(data=train, dictionary=None, corpus='gloss'):\n",
    "    import time\n",
    "    time_init = time.time()\n",
    "    count = 0\n",
    "    for i in data:\n",
    "        \n",
    "        lookup_dict = i.split(' | ')\n",
    "        target_word = lookup_dict[0]\n",
    "        target_sense = lookup_dict[1]\n",
    "        context = lookup_dict[2]\n",
    "\n",
    "        ## prediction based on Wordnet\n",
    "        if dictionary is None:\n",
    "            target_word = target_word.split('.')\n",
    "            a = leskSense_wn(context, target_word[0], pos=target_word[1])\n",
    "            if int(a.unicode_repr()[-4:-2]) == int(target_sense):\n",
    "                count+=1\n",
    "#                 print(target_word,\"wn_sense: \", (a.unicode_repr()[-4:-2]), \"train_sense: \",target_sense)\n",
    "\n",
    "        ## prediction based on given dictionary\n",
    "        else:\n",
    "            for j in dictionary.findAll('lexelt',{'item':target_word}):\n",
    "                a,b = leskSense_xml(context, target_word, j, corpus=corpus)\n",
    "                if str(a) == str(target_sense):\n",
    "                    count+=1\n",
    "#                     print(target_word,\"sense: \", a, \"train_sense: \",target_sense,\"max_overlaps: \", b)\n",
    "\n",
    "    print('Time Taken: ', time.time()/60 - time_init/60, \" minutes\")\n",
    "    return 100*count/float(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensePredictionTest(data, dictionary=None, corpus='gloss'):\n",
    "\n",
    "    sense_list=[]\n",
    "    for i in data:\n",
    "        lookup_dict = i.split(' | ')\n",
    "        target_word = lookup_dict[0]\n",
    "        target_sense = lookup_dict[1]\n",
    "        context = lookup_dict[2]\n",
    "        \n",
    "        ## prediction using Wordnet\n",
    "        if dictionary is None:\n",
    "            target_word = target_word.split('.')\n",
    "            a = leskSense_wn(context, target_word[0], pos=target_word[1])\n",
    "            sense_list.append(str(a.unicode_repr()[-4:-2]))\n",
    "            \n",
    "        ## prediction based on given dictionary\n",
    "        else:\n",
    "            for j in dictionary.findAll('lexelt',{'item':target_word}):\n",
    "                a,b = leskSense_xml(context, target_word, j, corpus=corpus)\n",
    "                sense_list.append(str(a))\n",
    "                \n",
    "    return sense_list\n",
    "\n",
    "def writeResults(sense_list, filename):\n",
    "    with open(filename, mode=\"w\") as outfile:  \n",
    "        for sense in sense_list:\n",
    "            outfile.write(\"%s\\n\" % sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Train Accuracy ===\n",
      "Time Taken:  1.80645370111  minutes\n",
      "Simple Lesk using definitions - Accuracy:  20.0666666667\n",
      "Time Taken:  2.31064834818  minutes\n",
      "Simple Lesk using examples - Accuracy:  49.2333333333\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    print(\"=== Train Accuracy ===\")\n",
    "    print(\"Simple Lesk using definitions - Accuracy: \",sensePrediction(train[:3000],dictionary=xml)) #this will be same for augDict as using definitions\n",
    "    print(\"Simple Lesk using examples - Accuracy: \",sensePrediction(train[:3000],dictionary=xml, corpus='examples'))\n",
    "    print(\"Corpus Lesk Accuracy: \",sensePrediction(train[:3000],dictionary=augXml, corpus='examples'))\n",
    "    print(\"Wordnet Accuracy: \",sensePrediction(train[:3000],dictionary=None))\n",
    "    \n",
    "    print(\"=== Vaidate Accuracy ===\")\n",
    "    print(\"Simple Lesk using definitions - Accuracy: \",sensePrediction(validate,dictionary=xml)) #this will be same for augDict as using definitions\n",
    "    print(\"Simple Lesk using examples - Accuracy: \",sensePrediction(validate,dictionary=xml, corpus='examples'))\n",
    "    print(\"Corpus Lesk Accuracy: \",sensePrediction(validate,dictionary=augXml, corpus='examples'))\n",
    "    print(\"Wordnet Accuracy: \",sensePrediction(validate,dictionary=None))\n",
    "    \n",
    "    print(\"=== Test Prediction ===\")\n",
    "    print(\"==Writing output for Simple/Corpus Lesk with definition\")\n",
    "    writeResults(sensePredictionTest(data = test, dictionary=xml, corpus='gloss'), filename='testPredictionGloss.txt')\n",
    "#     print(\"==Writing output for Simple Lesk with examples\")\n",
    "#     writeResults(sensePredictionTest(data = test, dictionary=xml, corpus='examples'), filename='testPredictionSimpleLeskExamples.txt')\n",
    "#     print(\"==Writing output for Corpus Lesk with examples\")\n",
    "#     writeResults(sensePredictionTest(data = test, dictionary=augXml, corpus='examples'), filename='testPredictionCorpusLeskExamples.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word.pos</th>\n",
       "      <th>sense</th>\n",
       "      <th>sense_prior</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>affect.v</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allow.v</td>\n",
       "      <td>1</td>\n",
       "      <td>0.907407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allow.v</td>\n",
       "      <td>2</td>\n",
       "      <td>0.092593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>announce.v</td>\n",
       "      <td>1</td>\n",
       "      <td>0.988636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>announce.v</td>\n",
       "      <td>2</td>\n",
       "      <td>0.011364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>approve.v</td>\n",
       "      <td>1</td>\n",
       "      <td>0.943396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>approve.v</td>\n",
       "      <td>2</td>\n",
       "      <td>0.056604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>area.n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.736196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>area.n</td>\n",
       "      <td>2</td>\n",
       "      <td>0.233129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>area.n</td>\n",
       "      <td>3</td>\n",
       "      <td>0.030675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ask.v</td>\n",
       "      <td>1</td>\n",
       "      <td>0.649425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ask.v</td>\n",
       "      <td>2</td>\n",
       "      <td>0.321839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ask.v</td>\n",
       "      <td>3</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ask.v</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ask.v</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ask.v</td>\n",
       "      <td>6</td>\n",
       "      <td>0.008621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>attempt.v</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>authority.n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>authority.n</td>\n",
       "      <td>3</td>\n",
       "      <td>0.511111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>authority.n</td>\n",
       "      <td>4</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>authority.n</td>\n",
       "      <td>5</td>\n",
       "      <td>0.044444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>avoid.v</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>base.n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>base.n</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>base.n</td>\n",
       "      <td>3</td>\n",
       "      <td>0.413043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>base.n</td>\n",
       "      <td>6</td>\n",
       "      <td>0.076087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>base.n</td>\n",
       "      <td>7</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>begin.v</td>\n",
       "      <td>1</td>\n",
       "      <td>0.605263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>begin.v</td>\n",
       "      <td>2</td>\n",
       "      <td>0.368421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>begin.v</td>\n",
       "      <td>3</td>\n",
       "      <td>0.008772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>state.n</td>\n",
       "      <td>2</td>\n",
       "      <td>0.831442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>state.n</td>\n",
       "      <td>3</td>\n",
       "      <td>0.038898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>system.n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.288889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>system.n</td>\n",
       "      <td>2</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>system.n</td>\n",
       "      <td>3</td>\n",
       "      <td>0.004444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>system.n</td>\n",
       "      <td>4</td>\n",
       "      <td>0.146667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>system.n</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>turn.v</td>\n",
       "      <td>1</td>\n",
       "      <td>0.526471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>turn.v</td>\n",
       "      <td>11</td>\n",
       "      <td>0.023529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>turn.v</td>\n",
       "      <td>12</td>\n",
       "      <td>0.002941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>turn.v</td>\n",
       "      <td>13</td>\n",
       "      <td>0.008824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>turn.v</td>\n",
       "      <td>14</td>\n",
       "      <td>0.002941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>turn.v</td>\n",
       "      <td>15</td>\n",
       "      <td>0.052941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>turn.v</td>\n",
       "      <td>2</td>\n",
       "      <td>0.132353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>turn.v</td>\n",
       "      <td>3</td>\n",
       "      <td>0.038235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>turn.v</td>\n",
       "      <td>4</td>\n",
       "      <td>0.052941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>turn.v</td>\n",
       "      <td>5</td>\n",
       "      <td>0.052941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>turn.v</td>\n",
       "      <td>6</td>\n",
       "      <td>0.094118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>turn.v</td>\n",
       "      <td>7</td>\n",
       "      <td>0.008824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>turn.v</td>\n",
       "      <td>8</td>\n",
       "      <td>0.002941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>value.n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.071642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>value.n</td>\n",
       "      <td>2</td>\n",
       "      <td>0.892537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>value.n</td>\n",
       "      <td>3</td>\n",
       "      <td>0.035821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>work.v</td>\n",
       "      <td>1</td>\n",
       "      <td>0.647826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>work.v</td>\n",
       "      <td>2</td>\n",
       "      <td>0.213043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>work.v</td>\n",
       "      <td>3</td>\n",
       "      <td>0.013043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>work.v</td>\n",
       "      <td>4</td>\n",
       "      <td>0.017391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>work.v</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>work.v</td>\n",
       "      <td>8</td>\n",
       "      <td>0.060870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>work.v</td>\n",
       "      <td>9</td>\n",
       "      <td>0.039130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        word.pos sense  sense_prior\n",
       "0       affect.v     1     1.000000\n",
       "1        allow.v     1     0.907407\n",
       "2        allow.v     2     0.092593\n",
       "3     announce.v     1     0.988636\n",
       "4     announce.v     2     0.011364\n",
       "5      approve.v     1     0.943396\n",
       "6      approve.v     2     0.056604\n",
       "7         area.n     1     0.736196\n",
       "8         area.n     2     0.233129\n",
       "9         area.n     3     0.030675\n",
       "10         ask.v     1     0.649425\n",
       "11         ask.v     2     0.321839\n",
       "12         ask.v     3     0.011494\n",
       "13         ask.v     4     0.002874\n",
       "14         ask.v     5     0.005747\n",
       "15         ask.v     6     0.008621\n",
       "16     attempt.v     1     1.000000\n",
       "17   authority.n     1     0.433333\n",
       "18   authority.n     3     0.511111\n",
       "19   authority.n     4     0.011111\n",
       "20   authority.n     5     0.044444\n",
       "21       avoid.v     1     1.000000\n",
       "22        base.n     1     0.250000\n",
       "23        base.n     2     0.010870\n",
       "24        base.n     3     0.413043\n",
       "25        base.n     6     0.076087\n",
       "26        base.n     7     0.250000\n",
       "27       begin.v     1     0.605263\n",
       "28       begin.v     2     0.368421\n",
       "29       begin.v     3     0.008772\n",
       "..           ...   ...          ...\n",
       "330      state.n     2     0.831442\n",
       "331      state.n     3     0.038898\n",
       "332     system.n     1     0.288889\n",
       "333     system.n     2     0.555556\n",
       "334     system.n     3     0.004444\n",
       "335     system.n     4     0.146667\n",
       "336     system.n     5     0.004444\n",
       "337       turn.v     1     0.526471\n",
       "338       turn.v    11     0.023529\n",
       "339       turn.v    12     0.002941\n",
       "340       turn.v    13     0.008824\n",
       "341       turn.v    14     0.002941\n",
       "342       turn.v    15     0.052941\n",
       "343       turn.v     2     0.132353\n",
       "344       turn.v     3     0.038235\n",
       "345       turn.v     4     0.052941\n",
       "346       turn.v     5     0.052941\n",
       "347       turn.v     6     0.094118\n",
       "348       turn.v     7     0.008824\n",
       "349       turn.v     8     0.002941\n",
       "350      value.n     1     0.071642\n",
       "351      value.n     2     0.892537\n",
       "352      value.n     3     0.035821\n",
       "353       work.v     1     0.647826\n",
       "354       work.v     2     0.213043\n",
       "355       work.v     3     0.013043\n",
       "356       work.v     4     0.017391\n",
       "357       work.v     5     0.008696\n",
       "358       work.v     8     0.060870\n",
       "359       work.v     9     0.039130\n",
       "\n",
       "[360 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "prob_s = [i.split(' | ') for i in train]\n",
    "df = pd.DataFrame(prob_s)\n",
    "df.columns = ['word.pos','sense','example']\n",
    "df['sense_prior'] = 1\n",
    "df1 = df.groupby(['word.pos','sense']).agg({'sense_prior': 'sum'})\n",
    "sense_priors = df1.groupby(level=0).apply(lambda x: x / float(x.sum()))\n",
    "sense_priors.reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Naive Bayes WSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_V(train):    \n",
    "    un_train = []\n",
    "    t = \"\"\n",
    "    a = []\n",
    "    for i in train[:1000]:\n",
    "            feature  = i.split(' | ')\n",
    "            t  = feature[0]\n",
    "            un_train.append(t)\n",
    "            [a.append(x) for x in un_train  if x not in a]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparingTrainingData(train,vocab):\n",
    "    docs = {}\n",
    "    words = {}\n",
    "    sense_count = []\n",
    "    index = 0 \n",
    "    idx = 0\n",
    "    temp = 0\n",
    "    for i in train:\n",
    "        feature = i.split(' | ')\n",
    "        train_word = feature[0]\n",
    "        if train_word == vocab:\n",
    "            train_sense = feature[1]\n",
    "            sense_count += train_sense\n",
    "            train_context = ''.join(feature[2].split('%% ')) # removing %% around target word\n",
    "            if train_word <> temp:\n",
    "                ct = {}\n",
    "                sense = []\n",
    "                temp = train_word\n",
    "                if train_sense not in sense:\n",
    "                    sense += train_sense\n",
    "                    words[train_word] = sense\n",
    "                    ct[train_sense] = ct.get(train_sense,'')+''+train_context\n",
    "                else:\n",
    "                    ct[train_sense] = ct.get(train_sense,'')+''+train_context\n",
    "            else:\n",
    "\n",
    "                if train_sense not in sense:\n",
    "                    sense += train_sense \n",
    "                    words[train_word] = sense\n",
    "                    ct[train_sense] = ct.get(train_sense,'')+''+train_context\n",
    "                else:\n",
    "                    ct[train_sense] = ct.get(train_sense,'')+''+train_context\n",
    "            docs[train_word] = ct\n",
    "    return docs,sense_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniquesensecount(sense_count):\n",
    "    sc = {}\n",
    "    usense = []\n",
    "    [usense.append(x) for x in sense_count if x not in usense]\n",
    "    usense\n",
    "    for s in sense_count:\n",
    "        sc[s]  = sense_count.count(s)\n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revise_documents(docs):\n",
    "    stemmer = PorterStemmer()\n",
    "    senses = {}     # {reference:sense}\n",
    "    count = 0 \n",
    "    ulist = []\n",
    "    b ={}\n",
    "    for word, l in docs.items():\n",
    "        for s, text in l.items():\n",
    "            words = re.findall(r\"[\\w']+\", text)\n",
    "            word_list = []\n",
    "            for w in words:\n",
    "                word_list.append(stemmer.stem(w.lower(),))\n",
    "            count += len(word_list)\n",
    "            #print(word_list)# get word count for the whole doc\n",
    "            b[s] = word_list\n",
    "            [ulist.append(x) for x in word_list if x not in ulist]\n",
    "    docs[word] = b\n",
    "    #print(l.get('1'))\n",
    "    count = len(ulist)\n",
    "    return docs, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revise_documents_test(docs_test):\n",
    "    stemmer = PorterStemmer()\n",
    "    senses = {}     # {reference:sense}\n",
    "    count = 0 \n",
    "    ulist = []\n",
    "    l ={}\n",
    "    for word, text in docs.items():\n",
    "        \n",
    "        words = re.findall(r\"[\\w']+\", str(text))\n",
    "        word_list = []\n",
    "        for w in words:\n",
    "            word_list.append(stemmer.stem(w.lower(),))\n",
    "        count += len(word_list)\n",
    "        #print(word_list)# get word count for the whole doc\n",
    "        l[s] = word_list\n",
    "        [ulist.append(x) for x in word_list if x not in ulist]\n",
    "    docs[word] = l\n",
    "    #print(l.get('1'))\n",
    "    count = len(ulist)\n",
    "    return docs, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_of_word_in_sense(word, context):\n",
    "    count = 0       # Returns count of given word in given sense\n",
    "    for w in context:      \n",
    "        if w == word:\n",
    "            count += 1\n",
    "    #print(\"count each word\",count)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_of_word_in_sense_test(word,sense_train,train_docs):\n",
    "    count = 0 \n",
    "    for word_train,sense in train_docs.items():\n",
    "        for s, test_text in sense.items():\n",
    "            for w in test_text:\n",
    "                if s == sense_train:\n",
    "                    if w == word:\n",
    "                        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(docs,count):\n",
    "    output = []\n",
    "    for word, senseList in docs.items():\n",
    "        V = count # Total count of words in train set\n",
    "        print(\"Variables \",V)\n",
    "        probabilities = {}\n",
    "        \n",
    "        for sense, context in senseList.items():\n",
    "            prob = float(sc.get(sense))/len(sense_count)  # P(class) prior\n",
    "            for word in context:       # P(word|sense) = count(word,sense) + 1 / count(sense) + V\n",
    "                # selected word count in given sense + 1 / all word count in given sense\n",
    "                p = (get_count_of_word_in_sense(word, context) + 1) / (float(len(context))+ V)\n",
    "                prob *=  p\n",
    "            probabilities[sense] = prob\n",
    "        best = max(probabilities, key=probabilities.get)\n",
    "        print(probabilities)\n",
    "        print(\"best\" +\" \" + str(best))\n",
    "        \n",
    "        #output.append(str(ref) + \" \" + str(best))\n",
    "    #print_txt(output, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_test(sense_train,test_docs,count,docs):\n",
    "    output = []\n",
    "    for word, context in test_docs.items():\n",
    "        V = count       \n",
    "        #print(\"Total count::::\",count, sense_train)\n",
    "       \n",
    "        #print(sc.get(sense_train),\" \",len(sense_count))\n",
    "        prob_test = float(sc.get(sense_train))/len(sense_count)\n",
    "        #print(\"sense probabilities:::\",prob_test)\n",
    "        for word in context:       # P(word|sense) = count(word,sense) + 1 / count(sense) + V\n",
    "            # selected word count in given sense + 1 / all word count in given sense\n",
    "            #print(\"word:::\",word)\n",
    "            p_test = (get_count_of_word_in_sense_test(word,sense_train,docs) + 1) / (float(len(context))+ V)\n",
    "            #print(\"print test\",p_test)\n",
    "            prob_test *=  p_test\n",
    "            #print(\"prob test\",prob_test)\n",
    "        probabilities_test= float(prob_test)\n",
    "        #best_test = max(probabilities_test, key=probabilities_test.get)\n",
    "        #output += best_test \n",
    "        \n",
    "    return probabilities_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_txt(item, path):\n",
    "    f = open(path, \"w\")\n",
    "    for i in item:\n",
    "        f.write(i + \"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    vocab  =  get_V(train)\n",
    "    print(vocab)\n",
    "    output = []\n",
    "    for v in vocab:\n",
    "        print(v)\n",
    "        docs,sense_count = preparingTrainingData(train,v)\n",
    "        sc = getUniquesensecount(sense_count)\n",
    "        print(\"sense \",sc )\n",
    "        docs,count = revise_documents(docs) #get total variables\n",
    "        #print(docs)\n",
    "        for i in test:\n",
    "            feature = i.split(' | ')\n",
    "            test_word = feature[0]\n",
    "            if feature[0] == v:\n",
    "                test_context = ''.join(feature[2].split('%% '))\n",
    "                docs_test[test_word] = test_context\n",
    "                docs_test,count       = revise_documents_test(docs_test)\n",
    "                prob = {}\n",
    "               \n",
    "                for s,n in sc.items():\n",
    "                    #for word, senseList in docs.items():\n",
    "                        #for sense, context in senseList.items():\n",
    "                            #print(\"how many sense:::\",sense)\n",
    "                    prob[s]=naive_bayes_test(s,docs_test,count,docs)\n",
    "                \n",
    "                best_test = max(prob, key=prob.get)\n",
    "                print(prob)\n",
    "                output.append(str(prob)+\" \"+str(test_word)+ \" best sense \"+ str(max(best_test)))\n",
    "    print_txt(output, \"ouput_naive\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
